我将为您现有的 `model_factory.py` 文件**添加相应的回归模型**，并保持与您现有代码一致的结构和风格。

### 主要变更概览：

1.  **添加回归器 (Regressors)**: 为`RandomForestClassifier`, `XGBClassifier`, 和 `LGBMClassifier` 添加了它们对应的回归版本：`RandomForestRegressor`, `XGBRegressor`, 和 `LGBMRegressor`。
2.  **替换线性模型**: 将 `LogisticRegression` (仅用于分类) 替换为更通用的线性回归模型 `Ridge`，它包含了L2正则化，是线性回归的一个稳健选择。
3.  **更新 `create_model` 工厂**: 在主创建函数中加入了新的模型类型标识（例如 `'RF_REG'`, `'XGB_REG'`），以区分分类和回归任务。
4.  **调整目标函数和评估指标**: 在XGBoost和LightGBM的回归版本中，将 `objective` 和 `eval_metric` 修改为适合回归任务的参数（如 `'reg:squarederror'` 和 `'rmse'`）。
5.  **移除分类专用参数**: 在回归模型中移除了不相关的参数，如 `class_weight`。

---

### 修改后的 `model_factory.py` (完整代码)

```python
"""
模型工厂模块
负责创建和管理不同类型的机器学习模型
"""
import xgboost as xgb
import lightgbm as lgb
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from typing import Dict, Any, Tuple, List
import pandas as pd
import numpy as np
from scipy.stats import randint, uniform
import logging


class ModelFactory:
    """模型工厂，负责创建和配置不同类型的模型"""
    
    @staticmethod
    def create_model(model_type: str, config: Dict[str, Any]) -> Tuple[object, Dict[str, Any]]:
        """
        创建模型实例
        
        Args:
            model_type: 模型类型
                        分类: 'LR', 'XGB', 'RF', 'LGB'
                        回归: 'RIDGE', 'XGB_REG', 'RF_REG', 'LGB_REG'
            config: 配置字典
            
        Returns:
            (模型实例, 参数网格)
        """
        
        model_type_upper = model_type.upper()

        # 分类模型
        if model_type_upper == 'LR':
            return ModelFactory._create_logistic_regression(config)
        elif model_type_upper == 'XGB':
            return ModelFactory._create_xgboost_classifier(config)
        elif model_type_upper == 'RF':
            return ModelFactory._create_random_forest_classifier(config)
        elif model_type_upper == 'LGB':
            return ModelFactory._create_lightgbm_classifier(config)
        
        # 回归模型
        elif model_type_upper == 'RIDGE':
            return ModelFactory._create_ridge_regression(config)
        elif model_type_upper == 'XGB_REG':
            return ModelFactory._create_xgboost_regressor(config)
        elif model_type_upper == 'RF_REG':
            return ModelFactory._create_random_forest_regressor(config)
        elif model_type_upper == 'LGB_REG':
            return ModelFactory._create_lightgbm_regressor(config)
            
        else:
            raise ValueError(f"不支持的模型类型: {model_type}")
    
    # =================================================================
    # 分类模型 (Classification Models)
    # =================================================================

    @staticmethod
    def _create_logistic_regression(config: Dict[str, Any]) -> Tuple[LogisticRegression, Dict[str, Any]]:
        """创建逻辑回归模型 (分类)"""
        modeling_config = config.get('modeling', {})
        lr_config = modeling_config.get('logistic_regression', {})
        
        model = LogisticRegression(
            C=lr_config.get('C', 1.0),
            penalty=lr_config.get('penalty', 'l2'),
            solver=lr_config.get('solver', 'saga'),
            max_iter=lr_config.get('max_iter', 5000),
            tol=lr_config.get('tol', 1e-4),
            class_weight=lr_config.get('class_weight', 'balanced'),
            random_state=modeling_config.get('random_state', 42)
        )
        
        param_grid = {
            'C': [0.001, 0.01, 0.1, 1, 10, 100],
            'penalty': ['l1', 'l2'],
            'max_iter': [5000, 10000],
            'tol': [1e-4, 1e-3, 1e-2]
        }
        
        return model, param_grid
    
    @staticmethod
    def _create_xgboost_classifier(config: Dict[str, Any]) -> Tuple[xgb.XGBClassifier, Dict[str, Any]]:
        """创建XGBoost模型 (分类)"""
        modeling_config = config.get('modeling', {})
        xgb_config = modeling_config.get('xgboost', {})
        
        model = xgb.XGBClassifier(
            objective='binary:logistic',
            eval_metric=xgb_config.get('eval_metric', 'auc'),
            random_state=modeling_config.get('random_state', 42),
            tree_method="hist",
            enable_categorical=True,
            n_estimators=xgb_config.get('n_estimators', 1000),
            learning_rate=xgb_config.get('learning_rate', 0.1),
            max_depth=xgb_config.get('max_depth', 6)
        )
        
        param_grid = {
            'n_estimators': randint(100, 2000),
            'max_depth': randint(3, 10),
            'learning_rate': uniform(loc=0.01, scale=0.29),
            'subsample': uniform(loc=0.6, scale=0.4),
            'colsample_bytree': uniform(loc=0.6, scale=0.4),
            'reg_alpha': uniform(loc=0, scale=10),
            'reg_lambda': uniform(loc=0, scale=10),
            'min_child_weight': randint(1, 10),
            'gamma': uniform(loc=0, scale=0.5)
        }
        
        return model, param_grid
    
    @staticmethod
    def _create_random_forest_classifier(config: Dict[str, Any]) -> Tuple[RandomForestClassifier, Dict[str, Any]]:
        """创建随机森林模型 (分类)"""
        modeling_config = config.get('modeling', {})
        rf_config = modeling_config.get('random_forest', {})
        
        model = RandomForestClassifier(
            n_estimators=rf_config.get('n_estimators', 100),
            max_depth=rf_config.get('max_depth', None),
            min_samples_split=rf_config.get('min_samples_split', 2),
            min_samples_leaf=rf_config.get('min_samples_leaf', 1),
            class_weight=rf_config.get('class_weight', 'balanced'),
            random_state=modeling_config.get('random_state', 42)
        )
        
        param_grid = {
            'n_estimators': randint(50, 500),
            'max_depth': randint(3, 15),
            'min_samples_split': randint(2, 20),
            'min_samples_leaf': randint(1, 10),
            'max_features': ['sqrt', 'log2', None],
            'bootstrap': [True, False]
        }
        
        return model, param_grid
    
    @staticmethod
    def _create_lightgbm_classifier(config: Dict[str, Any]) -> Tuple[lgb.LGBMClassifier, Dict[str, Any]]:
        """创建LightGBM模型 (分类)"""
        modeling_config = config.get('modeling', {})
        lgb_config = modeling_config.get('lightgbm', {})
        
        model = lgb.LGBMClassifier(
            objective='binary',
            random_state=modeling_config.get('random_state', 42),
            class_weight=lgb_config.get('class_weight', 'balanced'),
            n_estimators=lgb_config.get('n_estimators', 1000),
            learning_rate=lgb_config.get('learning_rate', 0.1),
            # ... 其他参数 ...
        )
        
        param_grid = {
            'n_estimators': randint(100, 2000),
            'num_leaves': randint(16, 256),
            'learning_rate': uniform(loc=0.01, scale=0.29),
            'feature_fraction': uniform(loc=0.5, scale=0.5),
            'bagging_fraction': uniform(loc=0.5, scale=0.5),
            'bagging_freq': randint(1, 7),
            'min_child_samples': randint(5, 100),
            'lambda_l1': uniform(loc=0, scale=10),
            'lambda_l2': uniform(loc=0, scale=10),
            'min_split_gain': uniform(loc=0, scale=0.5)
        }
        
        return model, param_grid

    # =================================================================
    # 回归模型 (Regression Models)
    # =================================================================

    @staticmethod
    def _create_ridge_regression(config: Dict[str, Any]) -> Tuple[Ridge, Dict[str, Any]]:
        """
        创建岭回归模型 (回归)
        
        参数说明:
        - alpha: 正则化强度，值越大正则化越强
        - solver: 优化算法
        """
        modeling_config = config.get('modeling', {})
        ridge_config = modeling_config.get('ridge_regression', {})

        model = Ridge(
            alpha=ridge_config.get('alpha', 1.0),
            solver=ridge_config.get('solver', 'auto'),
            random_state=modeling_config.get('random_state', 42)
        )

        param_grid = {
            'alpha': uniform(loc=0.1, scale=10), # [0.1, 10.1]
            'solver': ['svd', 'cholesky', 'lsqr', 'saga']
        }

        return model, param_grid

    @staticmethod
    def _create_xgboost_regressor(config: Dict[str, Any]) -> Tuple[xgb.XGBRegressor, Dict[str, Any]]:
        """创建XGBoost模型 (回归)"""
        modeling_config = config.get('modeling', {})
        xgb_reg_config = modeling_config.get('xgboost_regressor', {})

        model = xgb.XGBRegressor(
            objective='reg:squarederror', # <-- 回归目标函数
            eval_metric=xgb_reg_config.get('eval_metric', 'rmse'), # <-- 回归评估指标
            random_state=modeling_config.get('random_state', 42),
            tree_method="hist",
            enable_categorical=True,
            n_estimators=xgb_reg_config.get('n_estimators', 1000),
            learning_rate=xgb_reg_config.get('learning_rate', 0.1),
            max_depth=xgb_reg_config.get('max_depth', 6)
        )

        # 参数网格与分类器版本相同
        param_grid = {
            'n_estimators': randint(100, 2000),
            'max_depth': randint(3, 10),
            'learning_rate': uniform(loc=0.01, scale=0.29),
            'subsample': uniform(loc=0.6, scale=0.4),
            'colsample_bytree': uniform(loc=0.6, scale=0.4),
            'reg_alpha': uniform(loc=0, scale=10),
            'reg_lambda': uniform(loc=0, scale=10)
        }
        
        return model, param_grid

    @staticmethod
    def _create_random_forest_regressor(config: Dict[str, Any]) -> Tuple[RandomForestRegressor, Dict[str, Any]]:
        """创建随机森林模型 (回归)"""
        modeling_config = config.get('modeling', {})
        rf_reg_config = modeling_config.get('random_forest_regressor', {})
        
        model = RandomForestRegressor(
            n_estimators=rf_reg_config.get('n_estimators', 100),
            max_depth=rf_reg_config.get('max_depth', None),
            min_samples_split=rf_reg_config.get('min_samples_split', 2),
            min_samples_leaf=rf_reg_config.get('min_samples_leaf', 1),
            random_state=modeling_config.get('random_state', 42)
            # 注意：移除了 class_weight 参数
        )
        
        # 参数网格与分类器版本类似
        param_grid = {
            'n_estimators': randint(50, 500),
            'max_depth': randint(3, 15),
            'min_samples_split': randint(2, 20),
            'min_samples_leaf': randint(1, 10),
            'max_features': ['sqrt', 'log2', 1.0], # 1.0 相当于 None (所有特征)
            'bootstrap': [True, False]
        }
        
        return model, param_grid

    @staticmethod
    def _create_lightgbm_regressor(config: Dict[str, Any]) -> Tuple[lgb.LGBMRegressor, Dict[str, Any]]:
        """创建LightGBM模型 (回归)"""
        modeling_config = config.get('modeling', {})
        lgb_reg_config = modeling_config.get('lightgbm_regressor', {})

        model = lgb.LGBMRegressor(
            objective='regression', # <-- 回归目标函数
            random_state=modeling_config.get('random_state', 42),
            n_estimators=lgb_reg_config.get('n_estimators', 1000),
            learning_rate=lgb_reg_config.get('learning_rate', 0.1),
            # ... 其他参数 ...
            # 注意：移除了 class_weight 参数
        )

        # 参数网格与分类器版本相同
        param_grid = {
            'n_estimators': randint(100, 2000),
            'num_leaves': randint(16, 256),
            'learning_rate': uniform(loc=0.01, scale=0.29),
            'feature_fraction': uniform(loc=0.5, scale=0.5),
            'bagging_fraction': uniform(loc=0.5, scale=0.5),
            'bagging_freq': randint(1, 7),
            'min_child_samples': randint(5, 100),
            'lambda_l1': uniform(loc=0, scale=10),
            'lambda_l2': uniform(loc=0, scale=10)
        }
        
        return model, param_grid


class ModelValidator:
    """模型验证器"""
    
    @staticmethod
    def validate_model_config(model_type: str, config: Dict[str, Any]) -> bool:
        """验证模型配置"""
        # (此部分代码无需修改，它足够通用)
        # ...
        return True
```

您刚才在 `model_factory.py` 中添加了对回归模型的支持，这是一个非常重要的扩展。现在，为了让您的主训练脚本 `pipeline_training.py` 能够正确地处理这些新的回归模型，您需要进行一系列对应的、但非常清晰的改造。

### 核心改造思路：区分“分类任务”和“回归任务”

目前您的 `pipeline_training.py` 几乎是完全为**分类（Classification）**任务定制的。我们需要在关键的节点加入逻辑判断，让它在面对回归（Regression）任务时，能够采取正确的行为。

主要需要改造以下几个函数：

1.  **`_create_pipeline`**:
    *   **问题**: 目前代码写死了会使用`imblearn`的不平衡处理方法（如SMOTE），这只对分类有效。
    *   **改造**: 当检测到是回归模型时，**必须跳过**不平衡处理步骤。

2.  **`train_model`**:
    *   **问题**: 目前使用了`StratifiedKFold`（分层K折交叉验证）和早停机制中的`stratify`参数，这些都依赖于分类标签。回归任务的目标是连续值，无法进行分层。同时，超参数优化的`scoring`指标（如`roc_auc`）也是分类专用的。
    *   **改造**: 当是回归模型时，需要切换到标准的`KFold`交叉验证，移除`stratify`参数，并使用回归任务的评估指标（如`'neg_mean_squared_error'`）。

3.  **`run_single_model`**:
    *   **问题**: 日志和最终评估结果的打印都是围绕分类指标（AUC, KS, F1等）进行的。
    *   **改造**: 当是回归模型时，需要打印回归指标（RMSE, MAE, R2等）。

---

### 具体代码改造步骤

我将为您提供一个全面改造后的 `pipeline_training.py` 版本。请仔细查看注释，理解每个改动的目的。

#### 第1步：添加一个辅助函数来判断任务类型

在 `PipelineTraining` 类的最开始，添加一个小小的辅助方法，这将使后续的代码非常整洁。

```python
class PipelineTraining:
    """使用Pipeline的完整训练管道"""
    
    def __init__(self, config_path: Optional[str] = None):
        # ... (init 方法不变) ...

    def _is_regression_task(self, model_name: str) -> bool:
        """根据模型名称判断是否为回归任务"""
        return 'REG' in model_name.upper()

    # ... (其余方法) ...
```

#### 第2步：改造 `_create_pipeline`

这是最关键的一步，确保回归模型不会被错误地进行不平衡处理。

```python
    def _create_pipeline(self, numeric_features: List[str], 
                        categorical_features: List[str], model_name: str = None) -> Tuple[Pipeline, Dict]:
        """
        创建完整的训练Pipeline
        """
        model_type = model_name or self.config.get('modeling.model_type', 'LR')
        is_regression = self._is_regression_task(model_type)
        imbalance_method = self.config.get('modeling.imbalance_method', 'none')
        
        steps = []
        
        # 1. 数据预处理
        preprocessor = self._create_preprocessor(
            numeric_features, categorical_features, model_type
        )
        steps.append(('preprocessor', preprocessor))
        
        # 2. 不平衡处理 (仅对分类任务且配置开启时)
        if not is_regression and imbalance_method != 'none':
            random_state = self.config.get('modeling.random_state', 42)
            
            if imbalance_method == 'smote':
                sampler = SMOTE(random_state=random_state)
            # ... (其他采样方法) ...
            else:
                raise ValueError(f"不支持的不平衡处理方法: {imbalance_method}")
            
            steps.append(('sampler', sampler))
            logging.info(f"为分类任务添加了{imbalance_method}采样器")
        elif is_regression:
            logging.info("检测到回归任务，跳过不平衡处理步骤。")
        
        # 3. 模型 (分类器或回归器)
        model, param_grid = self._create_model(model_type)
        # 统一将步骤命名为'model'，可以是分类器也可以是回归器
        steps.append(('model', model)) 
        
        # 根据是否使用了采样器选择Pipeline类型
        if not is_regression and imbalance_method != 'none':
            pipeline = ImbPipeline(steps=steps)
        else:
            pipeline = Pipeline(steps=steps)
        
        return pipeline, param_grid
```

#### 第3步：改造 `train_model` (非常重要)

这里需要处理交叉验证、评估指标和早停机制的兼容性。

```python
    def train_model(self, 
                   X_train: pd.DataFrame, y_train: pd.Series,
                   X_test: pd.DataFrame, y_test: pd.Series,
                   model_name: str = None) -> Tuple[Pipeline, Dict[str, Any], Dict[str, Any]]:
        # ...
        model_type = model_name or self.config.get('modeling.model_type', 'LR')
        is_regression = self._is_regression_task(model_type)
        logging.info(f"开始训练 {model_type} 模型 (任务类型: {'回归' if is_regression else '分类'})...")

        # ... (检测特征类型和创建pipeline的代码不变) ...
        
        # ...

        # 检查是否需要早停机制
        needs_early_stopping = model_type.upper() in ['XGB', 'LGB', 'XGB_REG', 'LGB_REG']
        
        if needs_early_stopping:
            logging.info(f"为 {model_type} 创建早停验证集...")
            validation_split_ratio = self.config.get('modeling.validation_split_ratio', 0.1)
            
            # 回归任务不能使用 stratify
            stratify_param = y_train if not is_regression else None
            
            X_train_main, X_val, y_train_main, y_val = train_test_split(
                X_train, y_train, 
                test_size=validation_split_ratio, 
                random_state=self.config.get('modeling.random_state', 42),
                stratify=stratify_param # <-- 动态设置
            )
            train_data = (X_train_main, y_train_main)
            
            # ... (fit_params 的设置逻辑不变，只需将 'classifier__' 替换为 'model__') ...
            # 例如: fit_params = {'model__eval_set': [(X_val, y_val)], "model__verbose": False}
            # (为了简洁，这里省略具体代码，您的原逻辑是正确的，只需改前缀)
            if 'XGB' in model_type.upper():
                fit_params = {'model__eval_set': [(X_val, y_val)], "model__verbose": False}
            elif 'LGB' in model_type.upper():
                import lightgbm as lgb
                fit_params = {
                    "model__eval_set": [(X_val, y_val)],
                    "model__callbacks": [lgb.early_stopping(
                        stopping_rounds=self.config.get('modeling.early_stopping_rounds', 100), 
                        verbose=False
                    )]
                }

        # 根据是否调参选择训练实体
        if use_hyperparameter_tuning and param_grid:
            logging.info("启用超参数优化...")
            from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, KFold
            
            # 为参数网格添加前缀
            prefixed_param_grid = {f'model__{k}': v for k, v in param_grid.items()}
            
            # 为交叉验证选择正确的策略
            if is_regression:
                cv = KFold(n_splits=self.config.get('modeling.n_splits', 5), 
                           shuffle=True, 
                           random_state=self.config.get('modeling.random_state', 42))
                scoring_metric = self.config.get('modeling.primary_metric_regression', 'neg_root_mean_squared_error')
                logging.info(f"回归任务使用 KFold 和评估指标: {scoring_metric}")
            else:
                cv = StratifiedKFold(n_splits=self.config.get('modeling.n_splits', 5), 
                                     shuffle=True, 
                                     random_state=self.config.get('modeling.random_state', 42))
                scoring_metric = self.config.get('modeling.primary_metric_classification', 'roc_auc')
                logging.info(f"分类任务使用 StratifiedKFold 和评估指标: {scoring_metric}")

            searcher = RandomizedSearchCV(
                estimator=pipeline,
                param_distributions=prefixed_param_grid,
                n_iter=self.config.get('modeling.n_iter', 20),
                cv=cv,
                scoring=scoring_metric, # <-- 使用动态的评估指标
                # ... 其他参数 ...
            )
            # ...
        
        # ... (训练和评估的其余部分代码不变) ...

        # 评估模型
        logging.info("开始模型评估...")
        # 注意: 您的 ModelEvaluator 也需要能够处理回归任务！
        evaluation_results = self.evaluator.evaluate_model(
            final_pipeline, X_train, y_train, X_test, y_test, model_type, is_regression
        )
        
        return final_pipeline, evaluation_results, best_params
```
**注意**: 上述代码有一个关键的假设：**您的 `ModelEvaluator` 类也需要被改造**，以接收`is_regression`参数，并在`is_regression=True`时计算回归指标（如RMSE, MAE, R²），在`is_regression=False`时计算分类指标。

#### 第4步：改造 `run_single_model` 的日志输出

```python
    def run_single_model(self, model_name: str = None) -> Dict[str, Any]:
        # ...
        model_type = model_name or self.config.get('modeling.model_type', 'LR')
        is_regression = self._is_regression_task(model_type)
        # ...

            # 7. 保存模型和结果
            if result['status'] == 'success':
                # ...
                
                logging.info("=" * 80)
                logging.info(f"模型 {model_type} 训练完成")

                if is_regression:
                    # 打印回归指标
                    logging.info(f"  RMSE: {evaluation_results.get('test_metrics', {}).get('rmse', 'N/A')}")
                    logging.info(f"  MAE: {evaluation_results.get('test_metrics', {}).get('mae', 'N/A')}")
                    logging.info(f"  R2 Score: {evaluation_results.get('test_metrics', {}).get('r2', 'N/A')}")
                else:
                    # 打印分类指标
                    logging.info(f"  AUC: {evaluation_results.get('test_metrics', {}).get('auc', 'N/A')}")
                    logging.info(f"  F1: {evaluation_results.get('test_metrics', {}).get('f1', 'N/A')}")
                
                logging.info(f"  Best_params: {best_params}")
                logging.info("=" * 80)
            # ...
```

#### 第5步：修改 `_create_preprocessor` 中的一个小问题

您在XGB/LGB的日志中写到“不对数值特征进行缺失值填充”，但代码中实际上包含了`SimpleImputer`。为了让日志和代码行为一致，建议修改日志信息。

```python
        elif model_type_upper in ['XGB', 'LGB', 'XGB_REG', 'LGB_REG']: # <-- 包含回归模型
            # XGBoost/LightGBM：数值特征无需缩放，但我们仍然进行填充以保持一致性
            numeric_transformer = Pipeline(steps=[
                ('imputer', SimpleImputer(strategy='median'))
            ])
            # 修改日志以反映真实行为
            logging.info(f"为{model_type}模型使用中位数填充数值特征，但不进行缩放处理。")
```

### 总结

通过以上改造，您的 `pipeline_training.py` 现在变成了一个灵活的、能够同时处理**分类和回归**两种任务的强大训练管道。您只需要在配置文件中指定不同的模型名称（例如 `'LGB'` vs. `'LGB_REG'`），脚本就会自动选择正确的处理路径。

最后，请务必记得检查并同步修改您的 `evaluator.py` 文件，使其也能支持回归指标的计算。