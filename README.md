# æ˜“å—è¯ˆäººç¾¤è¯†åˆ«ç³»ç»Ÿ

ä¸€ä¸ªåŸºäºæœºå™¨å­¦ä¹ çš„æ™ºèƒ½åè¯ˆç³»ç»Ÿï¼Œæ”¯æŒæ¬ºè¯ˆæ£€æµ‹åˆ†ç±»ä»»åŠ¡å’Œæ¬ºè¯ˆæŸå¤±é‡‘é¢é¢„æµ‹å›å½’ä»»åŠ¡ã€‚

## ğŸ¯ é¡¹ç›®ç‰¹è‰²

- **åŒä»»åŠ¡æ”¯æŒ**ï¼šåŒæ—¶æ”¯æŒåˆ†ç±»ä»»åŠ¡ï¼ˆæ¬ºè¯ˆ/éæ¬ºè¯ˆè¯†åˆ«ï¼‰å’Œå›å½’ä»»åŠ¡ï¼ˆæ¬ºè¯ˆæŸå¤±é‡‘é¢é¢„æµ‹ï¼‰
- **å¤šæ¨¡å‹é›†æˆ**ï¼šé›†æˆRIDGEã€XGBoostã€éšæœºæ£®æ—ã€LightGBMå››ç§ä¸»æµæœºå™¨å­¦ä¹ æ¨¡å‹
- **å®Œæ•´å·¥ä½œæµ**ï¼šæ¶µç›–æ•°æ®é¢„å¤„ç†ã€ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹è®­ç»ƒã€è¯„ä¼°ã€éƒ¨ç½²å…¨æµç¨‹
- **çµæ´»é…ç½®**ï¼šé€šè¿‡YAMLé…ç½®æ–‡ä»¶çµæ´»æ§åˆ¶æ•´ä¸ªæœºå™¨å­¦ä¹ æµç¨‹
- **ç”Ÿäº§å°±ç»ª**ï¼šæ”¯æŒæ¨¡å‹å¯¼å‡ºã€PMMLè½¬æ¢ã€APIæœåŠ¡éƒ¨ç½²

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# æˆ–ä½¿ç”¨æœ€å°ä¾èµ–
pip install -r requirements_minimal.txt
```

### 2. è¿è¡Œåˆ†ç±»ä»»åŠ¡ï¼ˆæ¬ºè¯ˆæ£€æµ‹ï¼‰

```python
from src.pipeline_training import PipelineTraining

# ä½¿ç”¨é»˜è®¤é…ç½®è¿è¡Œåˆ†ç±»ä»»åŠ¡
pipeline = PipelineTraining()
results = pipeline.run()
```

### 3. è¿è¡Œå›å½’ä»»åŠ¡ï¼ˆæŸå¤±é‡‘é¢é¢„æµ‹ï¼‰

```python
from src.pipeline_training import PipelineTraining
from src.unified_config import ConfigManager

# åŠ è½½å›å½’é…ç½®
config = ConfigManager('config_regression.yaml')
pipeline = PipelineTraining(config)
results = pipeline.run()
```

## ğŸ“Š æ”¯æŒçš„æ¨¡å‹

### åˆ†ç±»æ¨¡å‹
- **LR** (Logistic Regression) - é€»è¾‘å›å½’
- **XGB** (XGBoost) - æç«¯æ¢¯åº¦æå‡
- **RF** (Random Forest) - éšæœºæ£®æ—
- **LGB** (LightGBM) - è½»é‡çº§æ¢¯åº¦æå‡
- **ET** (Extra Trees) - æç«¯éšæœºæ ‘
- **GBM** (Gradient Boosting) - æ¢¯åº¦æå‡
- **NB** (Naive Bayes) - æœ´ç´ è´å¶æ–¯
- **DT** (Decision Tree) - å†³ç­–æ ‘

### å›å½’æ¨¡å‹
- **RIDGE** (Ridge Regression) - å²­å›å½’
- **XGB_REG** (XGBoost Regression) - XGBoostå›å½’
- **RF_REG** (Random Forest Regression) - éšæœºæ£®æ—å›å½’
- **LGB_REG** (LightGBM Regression) - LightGBMå›å½’

## ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡

### åˆ†ç±»ä»»åŠ¡
- **Accuracy** (å‡†ç¡®ç‡)
- **Precision** (ç²¾ç¡®ç‡)
- **Recall** (å¬å›ç‡)
- **F1-Score** (F1åˆ†æ•°)
- **AUC-ROC** (ROCæ›²çº¿ä¸‹é¢ç§¯)
- **AUC-PR** (PRæ›²çº¿ä¸‹é¢ç§¯)

### å›å½’ä»»åŠ¡
- **MSE** (å‡æ–¹è¯¯å·®)
- **RMSE** (å‡æ–¹æ ¹è¯¯å·®)
- **MAE** (å¹³å‡ç»å¯¹è¯¯å·®)
- **RÂ²** (å†³å®šç³»æ•°)
- **MAPE** (å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®)

## âš™ï¸ é…ç½®æ–‡ä»¶

é¡¹ç›®ä½¿ç”¨YAMLé…ç½®æ–‡ä»¶ç®¡ç†ç³»ç»Ÿè¡Œä¸ºï¼š

- **config.yaml** - é»˜è®¤åˆ†ç±»ä»»åŠ¡é…ç½®
- **config_regression.yaml** - å›å½’ä»»åŠ¡ä¸“ç”¨é…ç½®
- **batch_config.yaml** - æ‰¹å¤„ç†ä»»åŠ¡é…ç½®

### å…³é”®é…ç½®é¡¹

```yaml
# ä»»åŠ¡ç±»å‹
task_type: "regression"  # æˆ– "classification"

# ç›®æ ‡å˜é‡
target_column: "fraud_loss_amount"  # å›å½’ä»»åŠ¡
# target_column: "is_fraud"  # åˆ†ç±»ä»»åŠ¡

# æ¨¡å‹é€‰æ‹©
models: ["LR", "XGB", "RF", "LGB"]  # åˆ†ç±»æ¨¡å‹
regression_models: ["RIDGE", "XGB_REG", "RF_REG", "LGB_REG"]  # å›å½’æ¨¡å‹
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
æ˜“å—è¯ˆäººç¾¤è¯†åˆ«/
â”œâ”€â”€ src/                    # æ ¸å¿ƒæºç 
â”‚   â”œâ”€â”€ pipeline_training.py # è®­ç»ƒç®¡é“
â”‚   â”œâ”€â”€ model_factory.py    # æ¨¡å‹å·¥å‚
â”‚   â”œâ”€â”€ evaluator.py        # è¯„ä¼°å™¨
â”‚   â”œâ”€â”€ feature_engineering.py # ç‰¹å¾å·¥ç¨‹
â”‚   â””â”€â”€ ...
â”œâ”€â”€ examples/               # ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ data/                   # æ•°æ®æ–‡ä»¶
â”œâ”€â”€ models/                 # è®­ç»ƒå¥½çš„æ¨¡å‹
â”œâ”€â”€ plots/                  # å¯è§†åŒ–å›¾è¡¨
â”œâ”€â”€ logs/                   # è¿è¡Œæ—¥å¿—
â”œâ”€â”€ config.yaml             # é…ç½®æ–‡ä»¶
â””â”€â”€ README.md              # é¡¹ç›®è¯´æ˜
```

## ğŸ› ï¸ é«˜çº§ç”¨æ³•

### æ–°çš„ç‰¹å¾å·¥ç¨‹Pipeline

é¡¹ç›®æ–°å¢äº†ç¬¦åˆsklearnè§„èŒƒçš„ç‰¹å¾å·¥ç¨‹Pipelineç±»ï¼š

#### 1. ç‰¹å¾é€‰æ‹©Pipeline
```python
from src.feature_engineering import FeatureSelectorPipeline

# åˆ›å»ºç‰¹å¾é€‰æ‹©å™¨
selector = FeatureSelectorPipeline(
    task_type='classification',
    selection_methods=['missing_rate', 'variance', 'correlation', 'importance'],
    missing_rate_threshold=0.3,
    importance_threshold=0.01
)

# è®­ç»ƒå¹¶åº”ç”¨
selector.fit(X_train, y_train)
X_selected = selector.transform(X_test)
```

#### 2. ç‰¹å¾ç”ŸæˆPipeline
```python
from src.feature_engineering import FeatureGeneratorPipeline

# åˆ›å»ºç‰¹å¾ç”Ÿæˆå™¨
generator = FeatureGeneratorPipeline(
    generate_polynomial=True,
    polynomial_degree=2,
    generate_interaction=True,
    generate_binning=True,
    bins_config={'age': 5, 'income': 4}
)

# ç”Ÿæˆæ–°ç‰¹å¾
X_generated = generator.transform(X)
```

#### 3. å®Œæ•´å·¥ä½œæµç¨‹
```python
from sklearn.pipeline import Pipeline

full_pipeline = Pipeline([
    ('generator', FeatureGeneratorPipeline(
        generate_polynomial=True,
        generate_interaction=True,
        generate_binning=True
    )),
    ('selector', FeatureSelectorPipeline(
        task_type='classification',
        selection_methods=['variance', 'importance']
    )),
    ('classifier', RandomForestClassifier())
])

full_pipeline.fit(X_train, y_train)
```

### è‡ªå®šä¹‰ç‰¹å¾å·¥ç¨‹

```python
from src.feature_engineering import FeatureEngineering

# è‡ªå®šä¹‰ç‰¹å¾å¤„ç†
fe = FeatureEngineering()
fe.custom_feature_engineering(data)
```

### æ¨¡å‹éƒ¨ç½²

```python
from src.model_deployer import ModelDeployer

# éƒ¨ç½²æ¨¡å‹ä¸ºAPIæœåŠ¡
deployer = ModelDeployer()
deployer.deploy_model(model_path, port=8000)
```

### æ‰¹é‡è®­ç»ƒ

```bash
# è¿è¡Œæ‰¹å¤„ç†è®­ç»ƒ
python batch_training.py
```

## ğŸ“š æ–‡æ¡£

- [é…ç½®æŒ‡å—](CONFIG_GUIDE.md) - è¯¦ç»†é…ç½®è¯´æ˜
- [å›å½’ä»»åŠ¡ä½¿ç”¨æŒ‡å—](å›å½’ä»»åŠ¡ä½¿ç”¨æŒ‡å—.md) - å›å½’ä»»åŠ¡ä¸“ç”¨æŒ‡å—
- [ä¼˜åŒ–æ”¹é€ å®ŒæˆæŠ¥å‘Š](ä¼˜åŒ–æ”¹é€ å®ŒæˆæŠ¥å‘Š.md) - é¡¹ç›®å‡çº§è®°å½•

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›é¡¹ç›®ï¼

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ - æŸ¥çœ‹[LICENSE](LICENSE)æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚