"""
æ‰¹é‡è®­ç»ƒè„šæœ¬ - æ”¯æŒåˆ†ç±»ä¸å›å½’åŒä»»åŠ¡çš„å¤šå‚æ•°ç»„åˆå’Œå¤šè¿›ç¨‹å¹¶è¡Œæ‰§è¡Œ

ä½¿ç”¨æ–¹æ³•:
    # åˆ†ç±»ä»»åŠ¡
    python batch_training.py --task classification --config batch_config.yaml
    python batch_training.py --task classification --config batch_config.yaml --dry-run
    
    # å›å½’ä»»åŠ¡
    python batch_training.py --task regression --config batch_config.yaml
    python batch_training.py --task regression --config batch_config.yaml --dry-run

åŠŸèƒ½ç‰¹ç‚¹:
    - æ”¯æŒåˆ†ç±»ä¸å›å½’ä»»åŠ¡é…ç½®
    - æ”¯æŒYAMLé…ç½®æ–‡ä»¶å®šä¹‰å¤šç»„å‚æ•°ç»„åˆ
    - å¤šè¿›ç¨‹å¹¶è¡Œæ‰§è¡Œï¼Œæé«˜è®­ç»ƒæ•ˆç‡
    - è¯¦ç»†çš„è¿›åº¦è·Ÿè¸ªå’Œç»“æœæ±‡æ€»
    - æ”¯æŒå¹²è¿è¡Œæ¨¡å¼é¢„è§ˆä»»åŠ¡
    - å¤±è´¥ä»»åŠ¡è‡ªåŠ¨é‡è¯•æœºåˆ¶
"""
import sys
import os
import yaml
import time
import json
import logging
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor, as_completed
from multiprocessing import cpu_count
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from src.pipeline_training import PipelineTraining
from src.unified_config import UnifiedConfig

# ç¡®ä¿logsç›®å½•å­˜åœ¨
logs_dir = os.path.join(os.path.dirname(__file__), 'logs')
os.makedirs(logs_dir, exist_ok=True)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(logs_dir, f'batch_training_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'), encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class BatchTrainer:
    """æ‰¹é‡è®­ç»ƒç®¡ç†å™¨"""
    
    def __init__(self, config_path):
        # ä»é…ç½®æ–‡ä»¶è¯»å–è®¾ç½®
        self.config = UnifiedConfig(config_path)
        
        # ä»é…ç½®æ–‡ä»¶è·å–å¹¶è¡Œè®¾ç½®
        self.max_workers = self.config.get('batch.max_workers', min(cpu_count(), 8))
        self.retry_times = self.config.get('batch.retry_times', 3)
        self.results = []
        
    def make_json_serializable(self, obj):
        """å°†å¯¹è±¡è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–çš„æ ¼å¼"""
        import numpy as np
        import pandas as pd
        
        if obj is None:
            return None
        elif isinstance(obj, (np.integer, np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, pd.Series):
            return obj.tolist()
        elif isinstance(obj, pd.DataFrame):
            return obj.to_dict('records')
        elif isinstance(obj, Path):
            return str(obj)
        elif isinstance(obj, dict):
            return {k: self.make_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self.make_json_serializable(item) for item in obj]
        elif hasattr(obj, 'items') and callable(getattr(obj, 'items')):
            # å¤„ç†ç±»ä¼¼å­—å…¸çš„å¯¹è±¡
            return {k: self.make_json_serializable(v) for k, v in obj.items()}
        elif hasattr(obj, '__class__') and 'sklearn' in str(type(obj)):
            # å¤„ç†sklearnå¯¹è±¡ï¼ˆå¦‚ColumnTransformerã€Pipelineç­‰ï¼‰
            return str(obj)
        else:
            return str(obj)
    
    def generate_config_combinations(self, parameter_grid):
        """æ ¹æ®å‚æ•°ç½‘æ ¼ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ"""
        from itertools import product
        import copy
        
        # ç”Ÿæˆæ‰€æœ‰ç»„åˆ
        keys = list(parameter_grid.keys())
        values = list(parameter_grid.values())
        
        combinations = []
        for combo in product(*values):
            # åˆ›å»ºå‚æ•°å­—å…¸
            params = {}
            for key, value in zip(keys, combo):
                # å¤„ç†åµŒå¥—é”®è·¯å¾„
                keys_path = key.split('.')
                current = params
                for k in keys_path[:-1]:
                    if k not in current:
                        current[k] = {}
                    current = current[k]
                current[keys_path[-1]] = value
            combinations.append(params)
            
        return combinations
    
    def create_config_file(self, config, output_dir, index):
        """åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶"""
        config_path = os.path.join(output_dir, f'temp_config_{index}.yaml')
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        return config_path
    
    def run_single_training(self, config_path, model_name, run_id):
        """æ‰§è¡Œå•ä¸ªè®­ç»ƒä»»åŠ¡"""
        try:
            logger.info(f"å¼€å§‹ä»»åŠ¡ {run_id}: æ¨¡å‹={model_name}, é…ç½®={config_path}")
            
            # ä½¿ç”¨é…ç½®æ–‡ä»¶åˆ›å»ºè®­ç»ƒç®¡é“
            pipeline = PipelineTraining(config_path=config_path)
            
            # å§‹ç»ˆè¿è¡ŒæŒ‡å®šæ¨¡å‹
            result = pipeline.run_single_model(model_name=model_name)
                
            # æ¸…ç†ç»“æœï¼Œç§»é™¤æ‰€æœ‰ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
            clean_result = {}
            for k, v in result.items():
                if k == 'model' or k == 'pipeline':
                    # è·³è¿‡æ¨¡å‹å’Œpipelineå¯¹è±¡
                    continue
                else:
                    # ä½¿ç”¨make_json_serializableæ¸…ç†å…¶ä»–å€¼
                    try:
                        clean_result[k] = self.make_json_serializable(v)
                    except Exception as e:
                        logger.warning(f"æ¸…ç†é”® '{k}' æ—¶å‡ºé”™: {str(e)}")
                        clean_result[k] = str(v)  # é™çº§ä¸ºå­—ç¬¦ä¸²
            
            # æ·»åŠ ä»»åŠ¡ä¿¡æ¯
            clean_result['run_id'] = run_id
            clean_result['config_path'] = str(config_path)  # ç¡®ä¿è·¯å¾„æ˜¯å­—ç¬¦ä¸²
            clean_result['model_name'] = model_name
            clean_result['timestamp'] = datetime.now().isoformat()
            
            logger.info(f"ä»»åŠ¡ {run_id} å®Œæˆ")
            return clean_result
            
        except Exception as e:
            logger.error(f"ä»»åŠ¡ {run_id} å¤±è´¥: {str(e)}")
            return {
                'status': 'failed',
                'error': str(e),
                'run_id': run_id,
                'config_path': str(config_path),
                'model_name': model_name,
                'timestamp': datetime.now().isoformat()
            }
    
    def run_batch_training(self, dry_run=False, task_type = None):
        """æ‰§è¡Œæ‰¹é‡è®­ç»ƒ"""
        
        # è·å–å‚æ•°ç½‘æ ¼
        parameter_grid = self.config.get('parameter_grid', {})
        logger.info(f"åŸå§‹å‚æ•°ç½‘æ ¼: {parameter_grid}")
        
        # è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
        enabled_models = self.config.get('models', [])
        logger.info(f"å¯ç”¨çš„æ¨¡å‹: {enabled_models}")
        
        # ç”Ÿæˆå‚æ•°ç»„åˆ
        param_combinations = self.generate_config_combinations(parameter_grid)
        logger.info(f"ç”Ÿæˆ {len(param_combinations)} ç»„å‚æ•°ç»„åˆ")
        
        # ç”Ÿæˆä»»åŠ¡åˆ—è¡¨
        tasks = []
        temp_dir = f'temp_configs_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
        os.makedirs(temp_dir, exist_ok=True)
        
        task_index = 0
        for model_name in enabled_models:
            for params in param_combinations:
                # åº”ç”¨å‚æ•°ç»„åˆåˆ°æ­£ç¡®çš„åµŒå¥—è·¯å¾„ - ä½¿ç”¨æ·±æ‹·è´é¿å…è¦†ç›–
                import copy
                temp_config = copy.deepcopy(self.config.to_dict())
                
                # ç§»é™¤æ‰¹å¤„ç†ç›¸å…³é…ç½®
                temp_config.pop('parameter_grid', None)
                temp_config.pop('models', None)
                
                # åº”ç”¨å‚æ•°ç»„åˆåˆ°æ­£ç¡®çš„åµŒå¥—è·¯å¾„
                def merge_nested_dict(d, key_path, value):
                    """åˆå¹¶åµŒå¥—å­—å…¸çš„å€¼ï¼Œæ”¯æŒå®Œæ•´å­—å…¸åˆå¹¶"""
                    if '.' in key_path:
                        # ç‚¹åˆ†è·¯å¾„ï¼Œå¦‚ modeling.imbalance_method
                        keys = key_path.split('.')
                        current = d
                        for key in keys[:-1]:
                            if key not in current:
                                current[key] = {}
                            current = current[key]
                        current[keys[-1]] = value
                    else:
                        # å®Œæ•´é”®ï¼Œå¦‚ modeling
                        if isinstance(value, dict):
                            # å¦‚æœæ˜¯å­—å…¸ï¼Œåˆå¹¶è€Œä¸æ˜¯è¦†ç›–
                            if key_path not in d:
                                d[key_path] = {}
                            d[key_path].update(value)
                        else:
                            # å¦‚æœä¸æ˜¯å­—å…¸ï¼Œç›´æ¥èµ‹å€¼
                            d[key_path] = value
                
                # åº”ç”¨æ¯ä¸ªå‚æ•°åˆ°æ­£ç¡®çš„ä½ç½®
                for key_path, value in params.items():
                    merge_nested_dict(temp_config, key_path, value)
                
                # è®¾ç½®æ¨¡å‹ç±»å‹ï¼ˆè¦†ç›–åŸæœ‰å€¼ï¼‰
                if 'modeling' not in temp_config:
                    temp_config['modeling'] = {}
                temp_config['modeling']['model_type'] = model_name
                
                # ä¿®å¤ï¼šç¡®ä¿pathsé…ç½®å®Œæ•´ï¼Œä½¿ç”¨UnifiedConfigçš„get_pathsæ–¹æ³•è·å–æ­£ç¡®è·¯å¾„
                if 'paths' not in temp_config:
                    temp_config['paths'] = {}
                
                # è·å–å½“å‰çš„ä¸å¹³è¡¡æ–¹æ³•
                imbalance_method = temp_config.get('modeling', {}).get('imbalance_method', 'none')
                
                # è·å–å½“å‰çš„æ•°æ®æ–‡ä»¶è·¯å¾„
                input_data = temp_config.get('paths', {}).get('input_data', None)
                
                # ä½¿ç”¨å½“å‰é…ç½®å€¼ç”Ÿæˆè·¯å¾„ï¼Œè€Œä¸æ˜¯åŸå§‹é…ç½®
                data_name = Path(input_data).stem
                
                # æ„å»ºè·¯å¾„é…ç½® - ç¡®ä¿æ¯ä¸ªä»»åŠ¡æœ‰ç‹¬ç«‹çš„æ—¥å¿—æ–‡ä»¶
                timestamp_suffix = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:17]  # åŒ…å«å¾®ç§’çš„é«˜ç²¾åº¦æ—¶é—´æˆ³
                paths = {
                    'data_path': 'data',
                    'input_data': input_data,
                    'log_output': f'logs/{data_name}_{imbalance_method}_{model_name}_{timestamp_suffix}_training.log',
                    'model_output': f'models/{data_name}_{imbalance_method}_{model_name}_model.joblib',
                    'plot_output': f'plots/{data_name}_{imbalance_method}_{model_name}_{{plot_type}}_plot.png'
                }
                
                temp_config['paths'].update(paths)
                
                # åŒæ—¶æ›´æ–°loggingé…ç½®ä»¥ç¡®ä¿ä¸€è‡´æ€§
                if 'logging' not in temp_config:
                    temp_config['logging'] = {}
                temp_config['logging']['file'] = paths['log_output']
                
                # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
                config_path = self.create_config_file(temp_config, temp_dir, task_index)
                run_id = f"{model_name}_{task_index}_{int(time.time())}"
                tasks.append({
                    'config_path': config_path,
                    'model_name': model_name,
                    'run_id': run_id
                })
                task_index += 1
        
        logger.info(f"å…±ç”Ÿæˆ {len(tasks)} ä¸ªè®­ç»ƒä»»åŠ¡")
        
        if dry_run:
            logger.info("å¹²è¿è¡Œæ¨¡å¼ - é¢„è§ˆä»»åŠ¡:")
            parameter_grid = self.config.get('parameter_grid', {})
            logger.info(f"å‚æ•°ç½‘æ ¼: {parameter_grid}")
            logger.info(f"ç”Ÿæˆçš„ç»„åˆæ•°é‡: {len(param_combinations)}")
            logger.info(f"æ€»ä»»åŠ¡æ•°é‡: {len(tasks)}")
            
            for i, task in enumerate(tasks):
                # è¯»å–é…ç½®æ–‡ä»¶å†…å®¹
                with open(task['config_path'], 'r', encoding='utf-8') as f:
                    config_content = yaml.safe_load(f)
                
                logger.info(f"ä»»åŠ¡ {i+1}/{len(tasks)}:")
                logger.info(f"  ä»»åŠ¡ID: {task['run_id']}")
                logger.info(f"  æ¨¡å‹: {task['model_name']}")
                logger.info(f"  æ•°æ®æ–‡ä»¶: {config_content.get('paths.input_data')}")
                logger.info(f"  ä¸å¹³è¡¡æ–¹æ³•: {config_content.get('modeling.imbalance_method')}")
                logger.info(f"  é…ç½®æ–‡ä»¶: {task['config_path']}")
                logger.info("  ---")
            return
        
        # æ‰§è¡Œå¹¶è¡Œè®­ç»ƒ
        start_time = time.time()
        completed_tasks = 0
        failed_tasks = 0
        
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_task = {
                executor.submit(self.run_single_training, task['config_path'], task['model_name'], task['run_id']): task
                for task in tasks
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_task):
                task = future_to_task[future]
                try:
                    result = future.result()
                    self.results.append(result)
                    
                    if result['status'] == 'success':
                        completed_tasks += 1
                    else:
                        failed_tasks += 1
                        
                    # å®æ—¶è¿›åº¦æŠ¥å‘Š
                    progress = (completed_tasks + failed_tasks) / len(tasks) * 100
                    logger.info(f"è¿›åº¦: {progress:.1f}% ({completed_tasks+failed_tasks}/{len(tasks)})")
                    
                except Exception as e:
                    logger.error(f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {str(e)}")
                    failed_tasks += 1
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        # import shutil
        # shutil.rmtree(temp_dir)
        
        # ç”Ÿæˆç»“æœæ±‡æ€»
        elapsed_time = time.time() - start_time
        self.generate_summary_report(elapsed_time, completed_tasks, failed_tasks, task_type)
        
        return self.results
    
    def generate_summary_report(self, elapsed_time, completed_tasks, failed_tasks, task_type):
        """ç”Ÿæˆè®­ç»ƒç»“æœæ±‡æ€»æŠ¥å‘Š"""
        
        # æ¸…ç†æ‰€æœ‰ç»“æœä¸­çš„ä¸å¯åºåˆ—åŒ–å¯¹è±¡
        clean_results = []
        for result in self.results:
            clean_result = {}
            for k, v in result.items():
                try:
                    clean_result[k] = self.make_json_serializable(v)
                except Exception as e:
                    logger.warning(f"æ¸…ç†ç»“æœä¸­çš„é”® '{k}' æ—¶å‡ºé”™: {str(e)}")
                    clean_result[k] = str(v)
            clean_results.append(clean_result)
        
        report = {
            'total_tasks': len(clean_results),
            'completed_tasks': completed_tasks,
            'failed_tasks': failed_tasks,
            'success_rate': completed_tasks / len(clean_results) if clean_results else 0,
            'elapsed_time': elapsed_time,
            'timestamp': datetime.now().isoformat(),
            'results': clean_results
        }
        
        # ä¿å­˜è¯¦ç»†ç»“æœåˆ°logsç›®å½•
        report_path = os.path.join(logs_dir, f'batch_training_results_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json')
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # æ‰“å°æ‘˜è¦
        logger.info("="*60)
        logger.info("æ‰¹é‡è®­ç»ƒå®Œæˆ!")
        logger.info(f"æ€»ä»»åŠ¡æ•°: {len(clean_results)}")
        logger.info(f"æˆåŠŸä»»åŠ¡: {completed_tasks}")
        logger.info(f"å¤±è´¥ä»»åŠ¡: {failed_tasks}")
        logger.info(f"æˆåŠŸç‡: {report['success_rate']:.2%}")
        logger.info(f"æ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
        logger.info(f"è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {report_path}")
        
        # ç”Ÿæˆå…¨å±€æ¨¡å‹æ¯”è¾ƒæŠ¥å‘Š
        self.generate_global_comparison_report(task_type)
        
        # æˆåŠŸä»»åŠ¡çš„ç»“æœæ‘˜è¦
        successful_results = [r for r in clean_results if r.get('status') == 'success']
        if successful_results:
            logger.info("\næˆåŠŸä»»åŠ¡æ‘˜è¦:")
            for result in successful_results:
                model = result.get('model_name', 'all')
                run_id = result['run_id']
                logger.info(f"  {run_id}: æ¨¡å‹={model}")
    
    def generate_global_comparison_report(self, task_type):
        """ç”Ÿæˆå…¨å±€æ¨¡å‹æ¯”è¾ƒæŠ¥å‘Š"""
        import pandas as pd
        
        logger.info("å¼€å§‹ç”Ÿæˆå…¨å±€æ¨¡å‹æ¯”è¾ƒæŠ¥å‘Š...")
        
        # æ”¶é›†æ‰€æœ‰æˆåŠŸçš„æ¨¡å‹ç»“æœ
        model_results = []
        models_dir = Path('models')
        
        # éå†æ‰€æœ‰ç»“æœæ–‡ä»¶
        for results_file in models_dir.glob('*_results.json'):
            try:
                with open(results_file, 'r', encoding='utf-8') as f:
                    result_data = json.load(f)
                
                # è§£ææ–‡ä»¶åè·å–æ¨¡å‹ä¿¡æ¯
                filename = results_file.stem  # å»æ‰.jsonå’Œ_model/results
                parts = filename.split('_')
                
                # æå–æ•°æ®åã€ä¸å¹³è¡¡æ–¹æ³•ã€æ¨¡å‹å
                if len(parts) > 6:
                    data_name = '_'.join(parts[-6:-4])  # åˆå¹¶å‰é¢çš„éƒ¨åˆ†
                    imbalance_method = '_'.join(parts[-4:-2])
                    model_name = parts[-2]
                else:
                    data_name = '_'.join(parts[-5:-3])  # åˆå¹¶å‰é¢çš„éƒ¨åˆ†
                    imbalance_method = parts[-3]
                    model_name = parts[-2]
                
                
                # æå–å…³é”®æŒ‡æ ‡ - æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©ä¸åŒçš„æŒ‡æ ‡
                evaluation_results = result_data.get('evaluation_results', {})
                test_metrics = evaluation_results.get('metrics', {}).get('test_metrics', {})
                
                # åŸºç¡€ä¿¡æ¯
                model_info = {
                    'data_name': data_name,
                    'imbalance_method': imbalance_method,
                    'model_name': model_name,
                    'task_type': task_type,
                    'file_path': str(results_file)
                }
                
                # æ ¹æ®ä»»åŠ¡ç±»å‹æå–ç›¸åº”æŒ‡æ ‡
                if task_type == 'regression':
                    # å›å½’ä»»åŠ¡æŒ‡æ ‡
                    model_info.update({
                        'rmse': test_metrics.get('rmse', 0),
                        'mae': test_metrics.get('mae', 0),
                        'r2': test_metrics.get('r2', 0),
                        'mse': test_metrics.get('mse', 0),
                        'mape': test_metrics.get('mape', 0)
                    })
                else:
                    # åˆ†ç±»ä»»åŠ¡æŒ‡æ ‡ï¼ˆé»˜è®¤ï¼‰
                    model_info.update({
                        'accuracy': test_metrics.get('accuracy', 0),
                        'precision': test_metrics.get('precision', 0),
                        'recall': test_metrics.get('recall', 0),
                        'f1': test_metrics.get('f1', 0),
                        'auc': test_metrics.get('auc', 0),
                        'ks': test_metrics.get('ks', 0)
                    })
                
                model_results.append(model_info)
                
            except Exception as e:
                logger.warning(f"è·³è¿‡æ–‡ä»¶ {results_file}: {str(e)}")
        
        if not model_results:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹ç»“æœæ–‡ä»¶ï¼Œæ— æ³•ç”Ÿæˆå…¨å±€æ¯”è¾ƒæŠ¥å‘Š")
            return
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(model_results)
        
        # æŒ‰ä»»åŠ¡ç±»å‹åˆ†ç»„å¤„ç†
        task_types = df['task_type'].unique()
        
        for task_type in task_types:
            task_df = df[df['task_type'] == task_type].copy()
            
            if task_type == 'regression':
                # å›å½’ä»»åŠ¡æŒ‰RÂ²æ’åºï¼ˆé™åºï¼‰
                task_df = task_df.sort_values('r2', ascending=False)
                sort_metric = 'r2'
            else:
                # åˆ†ç±»ä»»åŠ¡æŒ‰AUCæ’åºï¼ˆé™åºï¼‰
                task_df = task_df.sort_values('auc', ascending=False)
                sort_metric = 'auc'
            
            # ç”Ÿæˆæ—¶é—´æˆ³ç”¨äºæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ä¿å­˜ä¸ºCSVè¡¨æ ¼
            csv_path = os.path.join(logs_dir, f'global_model_comparison_{task_type}_{timestamp}.csv')
            task_df.to_csv(csv_path, index=False, encoding='utf-8')
            
            # æ‰“å°æŠ¥å‘Š
            logger.info(f"\n{'='*80}")
            logger.info(f"å…¨å±€æ¨¡å‹æ¯”è¾ƒæŠ¥å‘Š - {task_type.upper()}ä»»åŠ¡")
            logger.info(f"{'='*80}")
            
            # é€‰æ‹©è¦æ˜¾ç¤ºçš„åˆ—ï¼ˆæ ¹æ®ä»»åŠ¡ç±»å‹ï¼‰
            if task_type == 'regression':
                display_columns = ['data_name', 'imbalance_method', 'model_name', 'rmse', 'mae', 'r2', 'mape']
            else:
                display_columns = ['data_name', 'imbalance_method', 'model_name', 'accuracy', 'precision', 'recall', 'f1', 'auc', 'ks']
            
            display_df = task_df[display_columns]
            logger.info(f"\n{display_df.to_string(index=False)}")
            
            # ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
            summary_stats = {
                'total_models': len(task_df),
                'best_model': task_df.iloc[0]['model_name'] if len(task_df) > 0 else None,
                'data_coverage': task_df['data_name'].nunique(),
                'method_coverage': task_df['imbalance_method'].nunique(),
                'model_coverage': task_df['model_name'].nunique()
            }
            
            # æ·»åŠ æœ€ä½³æ¨¡å‹æŒ‡æ ‡
            if task_type == 'regression':
                summary_stats['best_model_r2'] = task_df.iloc[0]['r2'] if len(task_df) > 0 else 0
                summary_stats['best_model_rmse'] = task_df.iloc[0]['rmse'] if len(task_df) > 0 else 0
            else:
                summary_stats['best_model_auc'] = task_df.iloc[0]['auc'] if len(task_df) > 0 else 0
                summary_stats['best_model_ks'] = task_df.iloc[0]['ks'] if len(task_df) > 0 else 0
            
            logger.info(f"\n{task_type.upper()}ä»»åŠ¡æ±‡æ€»ç»Ÿè®¡:")
            logger.info(f"æ€»æ¨¡å‹æ•°: {summary_stats['total_models']}")
            logger.info(f"æœ€ä½³æ¨¡å‹: {summary_stats['best_model']}")
            
            if task_type == 'regression':
                logger.info(f"æœ€ä½³RÂ²: {summary_stats['best_model_r2']:.4f}")
                logger.info(f"æœ€ä½³RMSE: {summary_stats['best_model_rmse']:.4f}")
            else:
                logger.info(f"æœ€ä½³AUC: {summary_stats['best_model_auc']:.4f}")
                logger.info(f"æœ€ä½³KS: {summary_stats['best_model_ks']:.4f}")
            
            logger.info(f"è¦†ç›–æ•°æ®é›†: {summary_stats['data_coverage']} ä¸ª")
            logger.info(f"è¦†ç›–ä¸å¹³è¡¡å¤„ç†æ–¹æ³•: {summary_stats['method_coverage']} ä¸ª")
            logger.info(f"è¦†ç›–æ¨¡å‹ç±»å‹: {summary_stats['model_coverage']} ä¸ª")
            
            logger.info(f"\næŠ¥å‘Šæ–‡ä»¶å·²ä¿å­˜:")
            logger.info(f"ğŸ“Š {task_type.upper()}ä»»åŠ¡CSVè¡¨æ ¼: {csv_path}")
            logger.info("="*80)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='æ‰¹é‡è®­ç»ƒè„šæœ¬ - æ”¯æŒå¤šå‚æ•°ç»„åˆå’Œå¤šè¿›ç¨‹å¹¶è¡Œæ‰§è¡Œ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python batch_training.py --config batch_config.yaml
  python batch_training.py --config batch_config.yaml --dry-run
  python batch_training.py --help
        """
    )
    
    parser.add_argument(
        '--config', 
        type=str,
        help='æ‰¹é‡é…ç½®æ–‡ä»¶è·¯å¾„ (å°†æ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©)'
    )
    
    parser.add_argument(
        '--task', 
        type=str,
        choices=['classification', 'regression'],
        default='classification',
        help='ä»»åŠ¡ç±»å‹: classification(åˆ†ç±») æˆ– regression(å›å½’) (é»˜è®¤: classification)'
    )
    
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='å¹²è¿è¡Œæ¨¡å¼ï¼Œä»…é¢„è§ˆä»»åŠ¡ä¸æ‰§è¡Œ'
    )
    
    args = parser.parse_args()
    
    try:
        # ç¡®å®šé…ç½®æ–‡ä»¶
        if args.config:
            config_path = args.config
        elif args.task == 'regression':
            config_path = 'batch_config_regression.yaml'
        else:
            config_path = 'batch_config_classification.yaml'
        
        if not os.path.exists(config_path):
            logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return 1
            
        # åˆå§‹åŒ–æ‰¹é‡è®­ç»ƒå™¨
        trainer = BatchTrainer(config_path)
        
        # æ‰§è¡Œæ‰¹é‡è®­ç»ƒ
        trainer.run_batch_training(dry_run=args.dry_run, task_type = args.task)
        
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)